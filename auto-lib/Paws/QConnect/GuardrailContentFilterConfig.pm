# Generated by default/object.tt
package Paws::QConnect::GuardrailContentFilterConfig;
  use Moose;
  has InputStrength => (is => 'ro', isa => 'Str', request_name => 'inputStrength', traits => ['NameInRequest'], required => 1);
  has OutputStrength => (is => 'ro', isa => 'Str', request_name => 'outputStrength', traits => ['NameInRequest'], required => 1);
  has Type => (is => 'ro', isa => 'Str', request_name => 'type', traits => ['NameInRequest'], required => 1);

1;

### main pod documentation begin ###

=head1 NAME

Paws::QConnect::GuardrailContentFilterConfig

=head1 USAGE

This class represents one of two things:

=head3 Arguments in a call to a service

Use the attributes of this class as arguments to methods. You shouldn't make instances of this class. 
Each attribute should be used as a named argument in the calls that expect this type of object.

As an example, if Att1 is expected to be a Paws::QConnect::GuardrailContentFilterConfig object:

  $service_obj->Method(Att1 => { InputStrength => $value, ..., Type => $value  });

=head3 Results returned from an API call

Use accessors for each attribute. If Att1 is expected to be an Paws::QConnect::GuardrailContentFilterConfig object:

  $result = $service_obj->Method(...);
  $result->Att1->InputStrength

=head1 DESCRIPTION

Contains filter strengths for harmful content. AI Guardrail's support
the following content filters to detect and filter harmful user inputs
and FM-generated outputs.

=over

=item *

B<Hate>: Describes input prompts and model responses that discriminate,
criticize, insult, denounce, or dehumanize a person or group on the
basis of an identity (such as race, ethnicity, gender, religion, sexual
orientation, ability, and national origin).

=item *

B<Insults>: Describes input prompts and model responses that includes
demeaning, humiliating, mocking, insulting, or belittling language.
This type of language is also labeled as bullying.

=item *

B<Sexual>: Describes input prompts and model responses that indicates
sexual interest, activity, or arousal using direct or indirect
references to body parts, physical traits, or sex.

=item *

B<Violence>: Describes input prompts and model responses that includes
glorification of, or threats to inflict physical pain, hurt, or injury
toward a person, group, or thing.

=back

Content filtering depends on the confidence classification of user
inputs and FM responses across each of the four harmful categories. All
input and output statements are classified into one of four confidence
levels (NONE, LOW, MEDIUM, HIGH) for each harmful category. For
example, if a statement is classified as I<Hate> with HIGH confidence,
the likelihood of the statement representing hateful content is high. A
single statement can be classified across multiple categories with
varying confidence levels. For example, a single statement can be
classified as I<Hate> with HIGH confidence, I< Insults> with LOW
confidence, I<Sexual> with NONE confidence, and I<Violence> with MEDIUM
confidence.

=head1 ATTRIBUTES


=head2 B<REQUIRED> InputStrength => Str

The strength of the content filter to apply to prompts. As you increase
the filter strength, the likelihood of filtering harmful content
increases and the probability of seeing harmful content in your
application reduces.


=head2 B<REQUIRED> OutputStrength => Str

The strength of the content filter to apply to model responses. As you
increase the filter strength, the likelihood of filtering harmful
content increases and the probability of seeing harmful content in your
application reduces.


=head2 B<REQUIRED> Type => Str

The harmful category that the content filter is applied to.



=head1 SEE ALSO

This class forms part of L<Paws>, describing an object used in L<Paws::QConnect>

=head1 BUGS and CONTRIBUTIONS

The source code is located here: L<https://github.com/pplu/aws-sdk-perl>

Please report bugs to: L<https://github.com/pplu/aws-sdk-perl/issues>

=cut

