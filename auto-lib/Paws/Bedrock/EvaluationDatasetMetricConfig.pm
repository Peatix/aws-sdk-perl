# Generated by default/object.tt
package Paws::Bedrock::EvaluationDatasetMetricConfig;
  use Moose;
  has Dataset => (is => 'ro', isa => 'Paws::Bedrock::EvaluationDataset', request_name => 'dataset', traits => ['NameInRequest'], required => 1);
  has MetricNames => (is => 'ro', isa => 'ArrayRef[Str|Undef]', request_name => 'metricNames', traits => ['NameInRequest'], required => 1);
  has TaskType => (is => 'ro', isa => 'Str', request_name => 'taskType', traits => ['NameInRequest'], required => 1);

1;

### main pod documentation begin ###

=head1 NAME

Paws::Bedrock::EvaluationDatasetMetricConfig

=head1 USAGE

This class represents one of two things:

=head3 Arguments in a call to a service

Use the attributes of this class as arguments to methods. You shouldn't make instances of this class. 
Each attribute should be used as a named argument in the calls that expect this type of object.

As an example, if Att1 is expected to be a Paws::Bedrock::EvaluationDatasetMetricConfig object:

  $service_obj->Method(Att1 => { Dataset => $value, ..., TaskType => $value  });

=head3 Results returned from an API call

Use accessors for each attribute. If Att1 is expected to be an Paws::Bedrock::EvaluationDatasetMetricConfig object:

  $result = $service_obj->Method(...);
  $result->Att1->Dataset

=head1 DESCRIPTION

Defines the prompt datasets, built-in metric names and custom metric
names, and the task type.

=head1 ATTRIBUTES


=head2 B<REQUIRED> Dataset => L<Paws::Bedrock::EvaluationDataset>

Specifies the prompt dataset.


=head2 B<REQUIRED> MetricNames => ArrayRef[Str|Undef]

The names of the metrics you want to use for your evaluation job.

For knowledge base evaluation jobs that evaluate retrieval only, valid
values are "C<Builtin.ContextRelevance>", "C<Builtin.ContextCoverage>".

For knowledge base evaluation jobs that evaluate retrieval with
response generation, valid values are "C<Builtin.Correctness>",
"C<Builtin.Completeness>", "C<Builtin.Helpfulness>",
"C<Builtin.LogicalCoherence>", "C<Builtin.Faithfulness>",
"C<Builtin.Harmfulness>", "C<Builtin.Stereotyping>",
"C<Builtin.Refusal>".

For automated model evaluation jobs, valid values are
"C<Builtin.Accuracy>", "C<Builtin.Robustness>", and
"C<Builtin.Toxicity>". In model evaluation jobs that use a LLM as judge
you can specify "C<Builtin.Correctness>", "C<Builtin.Completeness">,
"C<Builtin.Faithfulness">, "C<Builtin.Helpfulness>",
"C<Builtin.Coherence>", "C<Builtin.Relevance>",
"C<Builtin.FollowingInstructions>",
"C<Builtin.ProfessionalStyleAndTone>", You can also specify the
following responsible AI related metrics only for model evaluation job
that use a LLM as judge "C<Builtin.Harmfulness>",
"C<Builtin.Stereotyping>", and "C<Builtin.Refusal>".

For human-based model evaluation jobs, the list of strings must match
the C<name> parameter specified in C<HumanEvaluationCustomMetric>.


=head2 B<REQUIRED> TaskType => Str

The the type of task you want to evaluate for your evaluation job. This
applies only to model evaluation jobs and is ignored for knowledge base
evaluation jobs.



=head1 SEE ALSO

This class forms part of L<Paws>, describing an object used in L<Paws::Bedrock>

=head1 BUGS and CONTRIBUTIONS

The source code is located here: L<https://github.com/pplu/aws-sdk-perl>

Please report bugs to: L<https://github.com/pplu/aws-sdk-perl/issues>

=cut

