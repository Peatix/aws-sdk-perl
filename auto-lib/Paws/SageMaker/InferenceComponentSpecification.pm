# Generated by default/object.tt
package Paws::SageMaker::InferenceComponentSpecification;
  use Moose;
  has BaseInferenceComponentName => (is => 'ro', isa => 'Str');
  has ComputeResourceRequirements => (is => 'ro', isa => 'Paws::SageMaker::InferenceComponentComputeResourceRequirements');
  has Container => (is => 'ro', isa => 'Paws::SageMaker::InferenceComponentContainerSpecification');
  has ModelName => (is => 'ro', isa => 'Str');
  has StartupParameters => (is => 'ro', isa => 'Paws::SageMaker::InferenceComponentStartupParameters');

1;

### main pod documentation begin ###

=head1 NAME

Paws::SageMaker::InferenceComponentSpecification

=head1 USAGE

This class represents one of two things:

=head3 Arguments in a call to a service

Use the attributes of this class as arguments to methods. You shouldn't make instances of this class. 
Each attribute should be used as a named argument in the calls that expect this type of object.

As an example, if Att1 is expected to be a Paws::SageMaker::InferenceComponentSpecification object:

  $service_obj->Method(Att1 => { BaseInferenceComponentName => $value, ..., StartupParameters => $value  });

=head3 Results returned from an API call

Use accessors for each attribute. If Att1 is expected to be an Paws::SageMaker::InferenceComponentSpecification object:

  $result = $service_obj->Method(...);
  $result->Att1->BaseInferenceComponentName

=head1 DESCRIPTION

Details about the resources to deploy with this inference component,
including the model, container, and compute resources.

=head1 ATTRIBUTES


=head2 BaseInferenceComponentName => Str

The name of an existing inference component that is to contain the
inference component that you're creating with your request.

Specify this parameter only if your request is meant to create an
adapter inference component. An adapter inference component contains
the path to an adapter model. The purpose of the adapter model is to
tailor the inference output of a base foundation model, which is hosted
by the base inference component. The adapter inference component uses
the compute resources that you assigned to the base inference
component.

When you create an adapter inference component, use the C<Container>
parameter to specify the location of the adapter artifacts. In the
parameter value, use the C<ArtifactUrl> parameter of the
C<InferenceComponentContainerSpecification> data type.

Before you can create an adapter inference component, you must have an
existing inference component that contains the foundation model that
you want to adapt.


=head2 ComputeResourceRequirements => L<Paws::SageMaker::InferenceComponentComputeResourceRequirements>

The compute resources allocated to run the model, plus any adapter
models, that you assign to the inference component.

Omit this parameter if your request is meant to create an adapter
inference component. An adapter inference component is loaded by a base
inference component, and it uses the compute resources of the base
inference component.


=head2 Container => L<Paws::SageMaker::InferenceComponentContainerSpecification>

Defines a container that provides the runtime environment for a model
that you deploy with an inference component.


=head2 ModelName => Str

The name of an existing SageMaker AI model object in your account that
you want to deploy with the inference component.


=head2 StartupParameters => L<Paws::SageMaker::InferenceComponentStartupParameters>

Settings that take effect while the model container starts up.



=head1 SEE ALSO

This class forms part of L<Paws>, describing an object used in L<Paws::SageMaker>

=head1 BUGS and CONTRIBUTIONS

The source code is located here: L<https://github.com/pplu/aws-sdk-perl>

Please report bugs to: L<https://github.com/pplu/aws-sdk-perl/issues>

=cut

